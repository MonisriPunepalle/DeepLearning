{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27091d74-42e7-4547-99f5-0be59d5f4982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 11:54:50.244629: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a5123c-646c-4f46-8b69-0c87f9476364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2022.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.0 huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e4744a-3d6f-4522-9ab4-6bea87f13a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(\"bert_input_105.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57fe823-67f0-432f-af59-e0cf3fd282a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing datasets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'].values, df['total_cost'].values, test_size=0.2)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize the texts and convert them to input IDs and attention masks\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeb7cef-9bef-45f9-831d-a213e15e00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
    "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(train_labels.astype(np.float32))\n",
    "\n",
    "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
    "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "val_labels = torch.tensor(val_labels.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197c9c84-b8c9-48ae-ab23-77b2c812c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training and validation datasets\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ddc78d-bc1b-4f7a-89f8-f78a40644bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917a8540-2ff8-4fbe-bbf9-f6c33795c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and the learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
    "# Define lists to store the validation loss and epochs\n",
    "val_loss_list = []\n",
    "epoch_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f318a7d9-bf2b-4b34-88d1-389a363c06be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tValidation Loss: 7023.3890380859375\n",
      "Epoch: 2 \tValidation Loss: 6885.93603515625\n",
      "Epoch: 3 \tValidation Loss: 6768.652587890625\n",
      "Epoch: 4 \tValidation Loss: 6671.2257080078125\n",
      "Epoch: 5 \tValidation Loss: 6588.869873046875\n",
      "Epoch: 6 \tValidation Loss: 6512.185791015625\n",
      "Epoch: 7 \tValidation Loss: 6432.440185546875\n",
      "Epoch: 8 \tValidation Loss: 6366.9461669921875\n",
      "Epoch: 9 \tValidation Loss: 6301.5145263671875\n",
      "Epoch: 10 \tValidation Loss: 6250.058349609375\n",
      "Epoch: 11 \tValidation Loss: 6191.85302734375\n",
      "Epoch: 12 \tValidation Loss: 6153.570068359375\n",
      "Epoch: 13 \tValidation Loss: 6115.400146484375\n",
      "Epoch: 14 \tValidation Loss: 6069.88037109375\n",
      "Epoch: 15 \tValidation Loss: 6041.6546630859375\n",
      "Epoch: 16 \tValidation Loss: 6014.661865234375\n",
      "Epoch: 17 \tValidation Loss: 5980.7503662109375\n",
      "Epoch: 18 \tValidation Loss: 5957.921875\n",
      "Epoch: 19 \tValidation Loss: 5935.67578125\n",
      "Epoch: 20 \tValidation Loss: 5914.7811279296875\n",
      "Epoch: 21 \tValidation Loss: 5900.2841796875\n",
      "Epoch: 22 \tValidation Loss: 5879.704345703125\n",
      "Epoch: 23 \tValidation Loss: 5857.1796875\n",
      "Epoch: 24 \tValidation Loss: 5841.2789306640625\n",
      "Epoch: 25 \tValidation Loss: 5824.2669677734375\n",
      "Epoch: 26 \tValidation Loss: 5807.9495849609375\n",
      "Epoch: 27 \tValidation Loss: 5792.2523193359375\n",
      "Epoch: 28 \tValidation Loss: 5777.4884033203125\n",
      "Epoch: 29 \tValidation Loss: 5760.851257324219\n",
      "Epoch: 30 \tValidation Loss: 5746.760437011719\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch in val_dataloader:\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            total_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "        val_loss_list.append(avg_loss)\n",
    "        epoch_list.append(epoch+1)\n",
    "        print('Epoch:', epoch+1, '\\tValidation Loss:', avg_loss)\n",
    "# Define the device to run the code\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77b73673-9867-4d21-a34e-57c77ff84dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 67.3558\n",
      "MSE: 16671.0888\n",
      "RMSE: 129.1166\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_texts = df['text'].values\n",
    "    test_labels = df['total_cost'].values\n",
    "    \n",
    "    test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=256)\n",
    "    \n",
    "    test_inputs = torch.tensor(test_encodings['input_ids'])\n",
    "    test_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "    test_labels = torch.tensor(test_labels.astype(np.float32))\n",
    "    \n",
    "    # Create a DataLoader for the test set\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=8)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    total_cost_predictions = []\n",
    "    total_cost_labels = []\n",
    "    \n",
    "    for batch in test_dataloader:\n",
    "        batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Compute logits and probabilities\n",
    "        batch_logits = model(batch_inputs, attention_mask=batch_masks)[0]\n",
    "        batch_probs = torch.sigmoid(batch_logits)\n",
    "        \n",
    "        # Collect predictions and labels\n",
    "        total_cost_predictions += batch_probs.flatten().tolist()\n",
    "        total_cost_labels += batch_labels.flatten().tolist()\n",
    "    \n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    total_cost_predictions = np.array(total_cost_predictions)\n",
    "    total_cost_labels = np.array(total_cost_labels)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    mae = np.mean(np.abs(total_cost_predictions - total_cost_labels))\n",
    "    mse = np.mean((total_cost_predictions - total_cost_labels) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "  \n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print('MAE: {:.4f}'.format(mae))\n",
    "    print('MSE: {:.4f}'.format(mse))\n",
    "    print('RMSE: {:.4f}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4eaf77-f5d0-405d-8347-828a4e6f688e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the validation loss over epochs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mepoch_list\u001b[49m, val_loss_list, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss over Epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the validation loss over epochs\n",
    "\n",
    "plt.plot(epoch_list, val_loss_list, 'b', label='Validation Loss')\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
